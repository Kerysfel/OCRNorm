dataset:
  name: BLN600
  path: TMP/bln600

llm:
  use_llm: true
  model_name: "llama-2-13b-chat"
  provider: local
  host: http://localhost:1234/v1/chat/completions
  model: llama-2-13b-chat
  temperature: 0.2
  max_tokens: 4096
  top_p: 0.7
  top_k: 50
  strategy: correction

experiment:
  output_path: results/bln600_llama_2_13b_correction.yaml
  debug: true
