## Datasets

В рамках данного проекта для задач нормализации OCR-текста мы используем несколько открытых датасетов. Из-за большого объёма и/или лицензионных ограничений сами данные **не** включены в репозиторий. Ниже приведены ссылки и краткие инструкции для их ручной загрузки.

### 1. IAM Handwritten Forms Dataset

- **Назначение**: Распознавание и постобработка рукописного английского текста.  
- **Ссылка**: [Kaggle: naderabdalghani/iam-handwritten-forms-dataset](https://www.kaggle.com/datasets/naderabdalghani/iam-handwritten-forms-dataset)  
- **Описание**: Содержит несколько тысяч отсканированных рукописных страниц. Часто используется для тестирования методов OCR и post-correction.  
- **Получение**:  
  - Зарегистрируйтесь на [Kaggle](https://www.kaggle.com/) (если ещё не сделали этого).  
  - Перейдите по ссылке датасета, примите условия и скачайте архив.  
  - Разместите извлечённые файлы в папку `data/raw/iam/`.

### 2. MLT19 (ICDAR 2019 Robust Reading Challenge)

- **Назначение**: Мультиязычные уличные надписи (English, Arabic, Korean, Japanese и др.), позволяют оценить, насколько OCR и LLM устойчивы к разным алфавитам и шумам.  
- **Официальная страница**: [ICDAR2019 Multi-lingual scene text detection and recognition](https://rrc.cvc.uab.es/?ch=15)  
- **Описание**: Состоит из ~10\,000 реальных изображений для обучения (разбитых на части), а также тестовых наборов. Предоставляет аннотации на уровне bounding box, скрипта и текста.  
- **Получение**:  
  - На сайте вы найдёте ссылки на Part1 / Part2 архивы (TrainSetImagesTask1), а также файлы с аннотациями.  
  - Скачайте вручную и поместите в `data/raw/mlt19/`.

### 3. Smile Twitter Emotion Dataset

- **Назначение**: Твиттер-корпус с эмоциональными метками; здесь может использоваться для экспериментов по нормализации коротких текстов и проверки качества обработки «социально-сетевых» ошибок/шумов.  
- **Ссылка**: [Kaggle: ashkhagan/smile-twitter-emotion-dataset](https://www.kaggle.com/datasets/ashkhagan/smile-twitter-emotion-dataset)  
- **Описание**: Содержит твиты, аннотированные по эмоциям. Может потребовать предварительной фильтрации/преобразований, прежде чем использовать в пайплайне OCR (если вы генерируете из них псевдо-сканы или вносите шум).  
- **Получение**:  
  - Скачайте архив через интерфейс Kaggle.  
  - Поместите содержимое в `data/raw/smile_twitter/`.

### 4. RuSentiment

- **Назначение**: Анализ русскоязычных текстов, аннотированных на предмет сентимента. Можно применять для экспериментов с нормализацией русского OCR, если вы создаёте синтетические документы из этих текстов.  
- **Репозиторий**: [GitHub: strawberrypie/rusentiment](https://github.com/strawberrypie/rusentiment)  
- **Описание**: Представляет собой датасет в формате TSV/CSV, где хранятся тексты и метки классов настроения.  
- **Получение**:  
  - Скачайте архив либо клонируйте репозиторий с GitHub.  
  - Распакуйте в `data/raw/rusentiment/`.

### 5. Gutenberg (публичные книги)

- **Назначение**: Классические литературные тексты (напр., на английском) для формирования синтетических PDF и последующего OCR. Полезны для тестирования нормализации на больших объёмах книжных текстов.  
- **Ссылка**: [Project Gutenberg](https://www.gutenberg.org/)  
- **Описание**: Предоставляет тексты многих книг в открытом доступе. Формат файлов: txt, epub.  
- **Получение**:  
  - На сайте найдите нужную книгу и скачайте, либо используйте прямые ссылки вида `https://www.gutenberg.org/files/<book_id>/<book_id>-0.txt`.  
  - Сохраните файл(ы) в `data/raw/gutenberg/`.

### 6. Lenta.ru (через Corus)

- **Назначение**: Русскоязычные новостные тексты. Можно взять небольшую выборку и превратить в PDF для OCR-экспериментов.  
- **Ссылка**: [Lenta.Ru-News-Dataset](https://github.com/yutkin/Lenta.Ru-News-Dataset)  
- **Описание**: CSV/TSV-файл, содержащий статьи, их заголовки и даты. Библиотека [Corus](https://github.com/natasha/corus) упрощает чтение этого формата.  
- **Получение**:  
  - Скачайте архив `lenta-ru-news.csv.gz` из раздела «Releases» в репо.  
  - Поместите в `data/raw/lenta/`.  
  - При необходимости используйте скрипт или код `from corus import load_lenta` для чтения записей.

---

## Как использовать датасеты в проекте

1. **Разместите файлы** в соответствующие подпапки внутри `data/raw/`.  
2. **Убедитесь**, что пути (например, `data/raw/iam/`) совпадают с теми, что вы указываете в настройках (конфигах) вашего проекта.  
3. **Предобработка и синтетические шумы** — при необходимости вы можете генерировать PDF, искусственно искажать тексты и т. д. для экспериментов по нормализации.