Ниже приведён **подробный план** последовательного наполнения репозитория и проведения экспериментов, включая практические детали (куда скачивать файлы, как проверять структуру данных, в каком порядке писать код и запускать), а также мысли о том, как это потом использовать в разделе \emph{Method} вашей статьи.

---

## Шаг 0. Инициализация репозитория и базовые настройки

1. **Создаём пустой репозиторий** (например, \texttt{ocr\_text\_normalization}) на GitHub (или локально).
2. В корне проекта размещаем:
   - \texttt{README.md} c описанием общей идеи и планируемой структуры.
   - \texttt{.gitignore}.
   - \texttt{requirements.txt} (или \texttt{environment.yml}), куда пока вписываем основные библиотеки (например, \texttt{requests}, \texttt{Levenshtein}, \texttt{sentence-transformers}, \texttt{PyYAML}, \texttt{pandas}, \texttt{nltk}).
3. В \texttt{launch.json} (если используете VSCode) создаём «заглушку» – конфигурации можно добавить позже, когда будет понятна структура скриптов.

На этом этапе в \texttt{README.md} достаточно иметь:
- Краткое описание: «Проект для нормализации OCR-текста через LLM».
- Как установить зависимости: 
  ```bash
  pip install -r requirements.txt
  ```

**Цель**: подготовить «скелет» проекта, который потом будем развивать. 

---

## Шаг 1. Организация данных

1. **Папка `data/`**:
   - Создайте подпапки: 
     - `raw/` — для исходных датасетов (или их примеров). 
     - `processed/` — для тех же датасетов после OCR/предобработки.  
     - `synthetic/` — для искусственно зашумлённых данных.  
     - `real_scans/` — для реальных сканов, если есть.  
   - В \texttt{data/README.md} пропишите (пока текстом), где брать реальные датасеты (IAM, MLT19 и т. д.), если они нужны.  
     - Например: «Скачайте IAM с \url{http://www.fki.inf.unibe.ch/databases/iam-handwriting-database} и поместите в \texttt{data/raw/iam/}.»

2. **Проверяем формат**: 
   - Допустим, IAM приходит в виде папок c изображениями рукописного текста, а MLT19 — в формате \texttt{.txt}+\texttt{.jpg}. Смотрим, где хранятся ground truth файлы. 
   - Решаем, как их будем «складывать» (возможно, делаем \texttt{data/raw/iam/} и \texttt{data/raw/mlt19/}).

---

## Шаг 2. Предварительное исследование формата (Data Exploration)

1. **Открываем пару файлов** (например, из IAM) и **понимаем**, в каком виде ground truth хранится: 
   - Часто это XML или текстовые транскрипты.  
   - MLT19 может иметь аннотации в CSV (текст + bounding box + язык).  
2. В \texttt{scripts/explore\_data.py} (или Jupyter-ноутбуке) делаем «быстрый» анализ: 
   - Считываем файлы, смотрим, сколько там записей, какие поля, какой язык.
   - Это поможет нам решить, **как** будем хранить данные в \texttt{processed/}.

3. **Формируем формат** «обработанных» данных. Например, JSON или TSV, где для каждой строки:
   ```json
   {
     "dataset_name": "IAM",
     "text_id": "iam_001",
     "language": "English",
     "original_text": "...",
     "ocr_text": null,  // пока нет OCR
     "noise_level": 0,  // пока нет шума
     "other_meta": ...
   }
   ```
   - Это пригодится для единообразия пайплайна.

**Цель**: понимать, в каком виде мы будем оперировать данными для экспериментов.

---

## Шаг 3. OCR-предобработка (если нужно)

1. **Создаём скрипт** \texttt{scripts/ocr\_process.py}:
   - Если нужно, запускаем Tesseract/EasyOCR на \texttt{data/raw/...} (сканы) и результат сохраняем в \texttt{data/processed/...}. 
   - В итоговом JSON/CSV храним колонки:
     - `original_text` (если есть GT), 
     - `ocr_text` (полученное от Tesseract/EasyOCR), 
     - `dataset_name`, 
     - др. метаданные.

2. **Проверяем** получившийся формат. Например, открываем \texttt{data/processed/iam\_ocr.json} и видим список записей.

**Цель**: получить «исходный OCR-вывод» для тех случаев, где ещё не было текстовых данных. Если \textbf{все} датасеты уже даны в виде «text ground-truth», можно пропустить этот шаг.

---

## Шаг 4. Генерация синтетических шумов (если планируете)

1. **Создаём** \texttt{scripts/generate\_synthetic\_noise.py}:
   - Читаем файл, например, \texttt{data/processed/iam\_ocr.json}.
   - Для каждой строки: «original\_text» → вносим 10% (или 20%, 30%) искажений (пропуски символов, замены, перестановки и т. д.).
   - Записываем результат в \texttt{data/synthetic/iam\_ocr\_10pct.json}, \texttt{iam\_ocr\_20pct.json} и т. д.

2. **Проверяем**:
   - Открываем несколько строк, смотрим, корректно ли зашумлён текст.  
   - Проверяем приблизительный CER (например, считаем \texttt{Levenshtein.distance / len(original\_text)}) и убеждаемся, что это ~10%, ~20%, ~30%.

**Цель**: теперь у нас есть зашумлённые тексты, где мы **знаем** исходную «чистую» версию. Это идеально подходит для проверки, насколько хорошо LLM восстанавливает.

---

## Шаг 5. Создание конфигов (YAML) и прописывание параметров

1. **В папке \texttt{config/}** заводим \texttt{default\_experiment.yaml}:
   ```yaml
   experiment_name: "default_experiment"
   language: "English"
   noise_levels: [0.1, 0.2, 0.3]
   prompt_strategy: "standard"
   ocr_engine: "tesseract"
   api_url: "http://localhost:1234/v1/chat/completions"
   model_id: "m7v03"
   output_path: "results/experiment_01"
   # ...
   ```
2. Если планируете «мультивью» (Tesseract+EasyOCR) или разные промпты, создавайте отдельные \texttt{.yaml} с другими названиями.

**Цель**: дать себе (и другим) удобный способ менять параметры экспериментов, не влезая в код.

---

## Шаг 6. Основной код пайплайна

### 6.1. `src/data_preprocessing.py`

- Функция чтения JSON/CSV, которая возвращает \texttt{List[Dict]} с записями вида:
  ```python
  [
    {
      "text_id": "iam_001",
      "original_text": "...",
      "noised_text": "...",
      "language": "English",
      "dataset_name": "IAM"
    },
    ...
  ]
  ```
- Опционально можно иметь функцию для разбиения на train/test, если планируется какое-то дообучение (в вашем случае, возможно, нет).

### 6.2. `src/prompt_strategies.py`

- Описываем **несколько** стратегий промптов (например, `Standard`, `Error-specific`, `MultiView`, etc.), оформляем их в словарь.
- Пример:
  ```python
  PROMPTS = {
      "standard": {
          "en": "Normalize the following text ...",
          "ru": "Приведите текст в соответствие ..."
      },
      "error_specific": {
          ...
      }
  }
  ```
- Функция \texttt{get\_prompt(strategy, lang)} возвращает строку prompt’а.

### 6.3. `src/normalization_pipeline.py`

- Принимает:
  ```python
  def normalize_text(noised_text: str, strategy: str, lang: str, model_id: str, api_url: str) -> str:
      # 1) берем prompt = get_prompt(strategy, lang)
      # 2) отправляем "prompt + noised_text" в API
      # 3) возвращаем ответ модели
  ```
- Вы можете в нём или в отдельном модуле хранить \texttt{send\_request\_to\_api} (HTTP-запрос к \texttt{api\_url}).

### 6.4. `src/evaluation.py`

- Функции для расчёта \texttt{CER}, \texttt{WER}, \texttt{semantic\_similarity}, etc.
  ```python
  def calculate_cer(ref: str, hyp: str) -> float:
      return Levenshtein.distance(ref, hyp) / len(ref) if ref else 1.0
  ```
- Результат можно вернуть в виде словаря:
  ```python
  {
    "CER": 0.12,
    "WER": 0.25,
    "semantic_sim": 0.91
  }
  ```

**Цель**: всё, что нужно для запуска эксперимента, теперь сосредоточено в \texttt{src/}.

---

## Шаг 7. Написание скриптов в `scripts/`

### 7.1. `run_inference.py`

1. **Считываем аргументы** (например, через \texttt{argparse}): `--config config/default_experiment.yaml`.  
2. **Загружаем** YAML (например, через `PyYAML`), получаем словарь параметров.  
3. **Обходим** нужные файлы в \texttt{data/synthetic/} (или `processed/`). Для каждого entry:  
   - `normalized_text = normalize_text(...)`  
   - `metrics = evaluate(original_text, normalized_text, ...)`  
   - Сохраняем всё в список.  
4. **Пишем** результаты в \texttt{results/experiment\_01/metrics.csv}, + можно сделать \texttt{.json} c более подробной информацией (prompt, время ответа, и т. д.).

В \texttt{launch.json} VSCode настраиваем «конфигурацию»:
```jsonc
{
  "name": "Exp1 - Default Config",
  "type": "python",
  "request": "launch",
  "program": "${workspaceFolder}/scripts/run_inference.py",
  "args": [
    "--config",
    "${workspaceFolder}/config/default_experiment.yaml"
  ],
  "console": "integratedTerminal"
}
```
Таким образом, можно в один клик запускать эксперимент №1.

---

## Шаг 8. Анализ результатов (скрипт `evaluate_metrics.py`)

Возможно, вы захотите **отдельный** скрипт, который собирает метрики из \texttt{results/experiment\_01/metrics.json} и выводит краткую статистику (средний CER, WER, std и т. д.). Либо можно всё встроить в \texttt{run_inference.py}. 

Главное — **где-то** у вас появляется таблица (например, \texttt{.csv}) с результатами по каждому образцу, чтобы потом в статье показывать «CER у нас упал с 0.20 до 0.05».

---

## Шаг 9. Проверка, доводка и «финальный» репозиторий

1. **Проверяем**:
   - Всё ли работает на «чистой» установке (новый виртуальный env)? 
   - Есть ли в \texttt{README.md} понятная инструкция:  
     1) Установите зависимости,  
     2) Скачайте/разместите датасеты,  
     3) Запустите \texttt{run\_inference.py --config ...},  
     4) Результаты в \texttt{results/...}.  

2. **Очищаем**/убираем «мусор»:
   - Не храним огромные файлы датасетов в GitHub (лучше ссылки).  
   - Логи и временные файлы игнорируем (\texttt{.gitignore}).

3. **Добавляем** примеры результатов в \texttt{README.md}:
   - Например, как выглядел \texttt{noised\_text} и как \texttt{normalized\_text} после LLM.

4. **Опционально**: если нужно, делаем короткие видео/скриншоты для визуализации.

**Итог**: ваш репозиторий готов к «публикации» (либо можно оставить приватным до написания статьи).

---

## Шаг 10. Использование для написания главы \emph{Method}

На момент, когда у вас всё заработает, вы уже имеете:

1. **Описание датасетов** (какие используем, зачем, их специфика).
2. **Описание внесения шума** (10%, 20%, 30% CER).
3. **Описание схемы экспериментов** (промпты, мульти-OCR, итерации).
4. **Архитектуру кода** (pipeline с отдельными этапами).

В разделе \emph{Method} статьи вы можете описывать:

- **Data**: «Для экспериментов используем англ. и рус. тексты: IAM, MLT19, синтетические RU/EN. Генерируем шумы X\%...».  
- **Normalization pipeline**: «Применяем модель \texttt{m7v03} (через локальный API). На вход LLM подаются зашумлённые тексты с prompt “…”».  
- **Evaluation**: «Считаем CER, WER, дополнительно используем semantic similarity. Все эксперименты проводим при температурах 0.3…».  
- **Implementation details**: «Реализовано на Python 3.10, public GitHub repository \url{https://github.com/user/ocr_text_normalization}, где размещён скрипт запуска \texttt{run\_inference.py}…».

Таким образом, у вас будет уже готовая реальная «методика» + ссылка на репозиторий для воспроизводимости.

---

## Итоговое резюме

1. **Порядок наполнения**: 
   - (1) Определяем, какие данные/датасеты есть, 
   - (2) Изучаем их структуру, 
   - (3) OCR-предобработка (если нужно), 
   - (4) Генерируем шумы, 
   - (5) Настраиваем конфиги, 
   - (6) Пишем основной код пайплайна, 
   - (7) Создаём скрипт запуска (\texttt{run_inference.py}), 
   - (8) Анализируем результаты, 
   - (9) Убираем лишнее, \textbf{делаем репо чистым}, 
   - (10) Пишем \emph{Method} в статье, указывая, что всё доступно в репозитории.

2. **После завершения** получится аккуратный репозиторий с ясной структурой, где внешние пользователи смогут:
   1. Установить зависимости, 
   2. Разместить датасеты (или запустить \texttt{download\_data.py}, если вы это реализуете), 
   3. Запустить конкретную конфигурацию (через \texttt{launch.json} или команду), 
   4. Посмотреть результаты в \texttt{results/}.

3. **В главе \emph{Method}** вы опишете общий пайплайн: «сначала датасеты → потом генерируем шум → обращаемся к LLM по HTTP → считаем метрики → смотрим улучшения». 

Таким образом, **к финалу** вы и получите:
- Полностью воспроизводимый проект (репозиторий),
- Чётко изложенную методику (для научной статьи),
- Возможность расширять эксперименты (добавляя разные модели, промпты, multi-view), не ломая текущую структуру.